你需要评估大型语言模型 (LLM) 的数值型响应准确性，通过将其输出与参考的标准答案进行比较。你的目标是判断LLM的数值响应是否与标准答案匹配，重点评估小数点后的精度。

## 评估步骤如下：

输入格式：你会收到一个包含三个键的JSON对象：

"query"：原始问题/用户的提问。
"model_response"：来自LLM的响应，里面包含字符串数值。该值为字符串表示的数值。
"ground_truth"：期望的正确数值答案。该值也是字符串表示的数值。

示例：

```json
{
  "query": "计算的最终结果是多少？",
  "model_response": "计算结果是123.00313",
  "ground_truth": "123.003"
}
```

## 评估标准：
- 数值匹配：将LLM的响应和标准答案中的数值部分提取出来。
- 精度处理：根据小数点后位数较少的那个数值，使用相同的小数位数对两个数进行四舍五入。
例如，模型输出是123.00313，标准答案是123.003，标准答案有3位小数，因此将模型响应四舍五入到3位小数。
- 数值比较：比较四舍五入后的模型响应和标准答案是否相等。
- 大小写不敏感：对数值的大小写不相关，但要确保数值比较过程中排除非数值部分的干扰。

## 步骤化推理：
- 首先，提取model_response和ground_truth中的数值部分。
- 然后，依据ground_truth的小数位数，将model_response中的数值进行相应位数的四舍五入。
- 最后，比较四舍五入后的两个数值是否匹配。
  - 如果匹配，返回1（正确），否则返回0（错误）。

## 输出格式：
- 你的输出应为最终的数值评估：
  - 如果模型的响应与标准答案匹配，则为1。
  - 如果模型的响应与标准答案不匹配，则为0。
    - 除了最终的数值评估，还需根据步骤化的推理提供简短的理由。描述完简短理由后，给出结果，例如：“所以最终的数值评估结果为1（正确）”或“所以最终的数值评估结果为0（错误）”。
    - 注意：你的响应不能使用"true"或"false"，必须是0或1。

## 输入

```json
{{inputJson}}
```